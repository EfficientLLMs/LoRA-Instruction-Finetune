{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grow 70M Pythia to 160M Pythia\n",
    "\n",
    "Model size table:\n",
    "\n",
    "| Params | n_layers\t| d_model |\tn_heads | d_head | Hugging Face Checkpoints |\n",
    "| ------ | -------- | ------- | ------- | ------ | ------------------------ |\n",
    "|   70M  |    6     |   512   |    8    |   64   | [Standard](https://huggingface.co/EleutherAI/pythia-70m) |\n",
    "|  160M  |    12    |   768   |    12   |   64   | [Standard](https://huggingface.co/EleutherAI/pythia-160m) |\n",
    "\n",
    "\n",
    "Based on the code in https://github.com/allenai/staged-training/blob/main/gpt_pretrain.py.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Pythia 70M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T14:53:14.256796Z",
     "start_time": "2024-03-12T14:53:14.242622Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vibhamasti/Personal/CMU/S24/Capstone/code/LoRA-Instruction-Finetune/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import re\n",
    "import copy\n",
    "import json\n",
    "import importlib\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'transformers' from '/Users/vibhamasti/Personal/CMU/S24/Capstone/code/LoRA-Instruction-Finetune/venv/lib/python3.11/site-packages/transformers/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(sys.modules['transformers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T14:51:11.367435Z",
     "start_time": "2024-03-12T14:51:11.361609Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/Users/vibhamasti/Personal/CMU/S24/Capstone/code/LoRA-Instruction-Finetune/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finish the following sentence:\\nRaindrops on roses\\n\\nI have a question for you.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_70m = GPTNeoXForCausalLM.from_pretrained(\n",
    "  \"EleutherAI/pythia-70m\",\n",
    "  cache_dir=\"../.cache/pythia-70m\",\n",
    ")\n",
    "\n",
    "tokenizer_70m = AutoTokenizer.from_pretrained(\n",
    "  \"EleutherAI/pythia-70m\",\n",
    "  cache_dir=\"../.cache/pythia-70m\",\n",
    ")\n",
    "\n",
    "inputs = tokenizer_70m(\"Finish the following sentence:\\nRaindrops on roses\", return_tensors=\"pt\")\n",
    "tokens = model_70m.generate(**inputs)\n",
    "tokenizer_70m.decode(tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use function preserving to grow the model to 160M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_70m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 512)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      ")\n",
      "gpt_neox GPTNeoXModel(\n",
      "  (embed_in): Embedding(50304, 512)\n",
      "  (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-5): 6 x GPTNeoXLayer(\n",
      "      (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (attention): GPTNeoXAttention(\n",
      "        (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "        (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (mlp): GPTNeoXMLP(\n",
      "        (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (act): GELUActivation()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "gpt_neox.embed_in Embedding(50304, 512)\n",
      "gpt_neox.emb_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers ModuleList(\n",
      "  (0-5): 6 x GPTNeoXLayer(\n",
      "    (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (attention): GPTNeoXAttention(\n",
      "      (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "      (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (mlp): GPTNeoXMLP(\n",
      "      (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (act): GELUActivation()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.0 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.0.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.0.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.0.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.0.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.0.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.0.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.0.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "gpt_neox.layers.0.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "gpt_neox.layers.0.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.0.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.0.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "gpt_neox.layers.0.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "gpt_neox.layers.0.mlp.act GELUActivation()\n",
      "gpt_neox.layers.1 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.1.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.1.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.1.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.1.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.1.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.1.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.1.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "gpt_neox.layers.1.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "gpt_neox.layers.1.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.1.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.1.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "gpt_neox.layers.1.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "gpt_neox.layers.1.mlp.act GELUActivation()\n",
      "gpt_neox.layers.2 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.2.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.2.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.2.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.2.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.2.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.2.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.2.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "gpt_neox.layers.2.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "gpt_neox.layers.2.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.2.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.2.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "gpt_neox.layers.2.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "gpt_neox.layers.2.mlp.act GELUActivation()\n",
      "gpt_neox.layers.3 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.3.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.3.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.3.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.3.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.3.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.3.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.3.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "gpt_neox.layers.3.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "gpt_neox.layers.3.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.3.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.3.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "gpt_neox.layers.3.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "gpt_neox.layers.3.mlp.act GELUActivation()\n",
      "gpt_neox.layers.4 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.4.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.4.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.4.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.4.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.4.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.4.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.4.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "gpt_neox.layers.4.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "gpt_neox.layers.4.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.4.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.4.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "gpt_neox.layers.4.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "gpt_neox.layers.4.mlp.act GELUActivation()\n",
      "gpt_neox.layers.5 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.5.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.5.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.5.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.5.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.5.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.5.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.5.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "gpt_neox.layers.5.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "gpt_neox.layers.5.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.5.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.5.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "gpt_neox.layers.5.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "gpt_neox.layers.5.mlp.act GELUActivation()\n",
      "gpt_neox.final_layer_norm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "embed_out Linear(in_features=512, out_features=50304, bias=False)\n"
     ]
    }
   ],
   "source": [
    "for name, module in model_70m.named_modules():\n",
    "  print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "wte Embedding(50257, 768)\n",
      "wpe Embedding(1024, 768)\n",
      "drop Dropout(p=0.1, inplace=False)\n",
      "h ModuleList(\n",
      "  (0-11): 12 x GPT2Block(\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (attn): GPT2Attention(\n",
      "      (c_attn): Conv1D()\n",
      "      (c_proj): Conv1D()\n",
      "      (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): GPT2MLP(\n",
      "      (c_fc): Conv1D()\n",
      "      (c_proj): Conv1D()\n",
      "      (act): NewGELUActivation()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "h.0 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "h.0.ln_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.0.attn GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.0.attn.c_attn Conv1D()\n",
      "h.0.attn.c_proj Conv1D()\n",
      "h.0.attn.attn_dropout Dropout(p=0.1, inplace=False)\n",
      "h.0.attn.resid_dropout Dropout(p=0.1, inplace=False)\n",
      "h.0.ln_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.0.mlp GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.0.mlp.c_fc Conv1D()\n",
      "h.0.mlp.c_proj Conv1D()\n",
      "h.0.mlp.act NewGELUActivation()\n",
      "h.0.mlp.dropout Dropout(p=0.1, inplace=False)\n",
      "h.1 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "h.1.ln_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.1.attn GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.1.attn.c_attn Conv1D()\n",
      "h.1.attn.c_proj Conv1D()\n",
      "h.1.attn.attn_dropout Dropout(p=0.1, inplace=False)\n",
      "h.1.attn.resid_dropout Dropout(p=0.1, inplace=False)\n",
      "h.1.ln_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.1.mlp GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.1.mlp.c_fc Conv1D()\n",
      "h.1.mlp.c_proj Conv1D()\n",
      "h.1.mlp.act NewGELUActivation()\n",
      "h.1.mlp.dropout Dropout(p=0.1, inplace=False)\n",
      "h.2 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "h.2.ln_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.2.attn GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.2.attn.c_attn Conv1D()\n",
      "h.2.attn.c_proj Conv1D()\n",
      "h.2.attn.attn_dropout Dropout(p=0.1, inplace=False)\n",
      "h.2.attn.resid_dropout Dropout(p=0.1, inplace=False)\n",
      "h.2.ln_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.2.mlp GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.2.mlp.c_fc Conv1D()\n",
      "h.2.mlp.c_proj Conv1D()\n",
      "h.2.mlp.act NewGELUActivation()\n",
      "h.2.mlp.dropout Dropout(p=0.1, inplace=False)\n",
      "h.3 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "h.3.ln_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.3.attn GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.3.attn.c_attn Conv1D()\n",
      "h.3.attn.c_proj Conv1D()\n",
      "h.3.attn.attn_dropout Dropout(p=0.1, inplace=False)\n",
      "h.3.attn.resid_dropout Dropout(p=0.1, inplace=False)\n",
      "h.3.ln_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.3.mlp GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.3.mlp.c_fc Conv1D()\n",
      "h.3.mlp.c_proj Conv1D()\n",
      "h.3.mlp.act NewGELUActivation()\n",
      "h.3.mlp.dropout Dropout(p=0.1, inplace=False)\n",
      "h.4 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "h.4.ln_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.4.attn GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.4.attn.c_attn Conv1D()\n",
      "h.4.attn.c_proj Conv1D()\n",
      "h.4.attn.attn_dropout Dropout(p=0.1, inplace=False)\n",
      "h.4.attn.resid_dropout Dropout(p=0.1, inplace=False)\n",
      "h.4.ln_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.4.mlp GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.4.mlp.c_fc Conv1D()\n",
      "h.4.mlp.c_proj Conv1D()\n",
      "h.4.mlp.act NewGELUActivation()\n",
      "h.4.mlp.dropout Dropout(p=0.1, inplace=False)\n",
      "h.5 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "h.5.ln_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.5.attn GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.5.attn.c_attn Conv1D()\n",
      "h.5.attn.c_proj Conv1D()\n",
      "h.5.attn.attn_dropout Dropout(p=0.1, inplace=False)\n",
      "h.5.attn.resid_dropout Dropout(p=0.1, inplace=False)\n",
      "h.5.ln_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.5.mlp GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.5.mlp.c_fc Conv1D()\n",
      "h.5.mlp.c_proj Conv1D()\n",
      "h.5.mlp.act NewGELUActivation()\n",
      "h.5.mlp.dropout Dropout(p=0.1, inplace=False)\n",
      "h.6 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "h.6.ln_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.6.attn GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.6.attn.c_attn Conv1D()\n",
      "h.6.attn.c_proj Conv1D()\n",
      "h.6.attn.attn_dropout Dropout(p=0.1, inplace=False)\n",
      "h.6.attn.resid_dropout Dropout(p=0.1, inplace=False)\n",
      "h.6.ln_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.6.mlp GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.6.mlp.c_fc Conv1D()\n",
      "h.6.mlp.c_proj Conv1D()\n",
      "h.6.mlp.act NewGELUActivation()\n",
      "h.6.mlp.dropout Dropout(p=0.1, inplace=False)\n",
      "h.7 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "h.7.ln_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.7.attn GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.7.attn.c_attn Conv1D()\n",
      "h.7.attn.c_proj Conv1D()\n",
      "h.7.attn.attn_dropout Dropout(p=0.1, inplace=False)\n",
      "h.7.attn.resid_dropout Dropout(p=0.1, inplace=False)\n",
      "h.7.ln_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.7.mlp GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.7.mlp.c_fc Conv1D()\n",
      "h.7.mlp.c_proj Conv1D()\n",
      "h.7.mlp.act NewGELUActivation()\n",
      "h.7.mlp.dropout Dropout(p=0.1, inplace=False)\n",
      "h.8 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "h.8.ln_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.8.attn GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.8.attn.c_attn Conv1D()\n",
      "h.8.attn.c_proj Conv1D()\n",
      "h.8.attn.attn_dropout Dropout(p=0.1, inplace=False)\n",
      "h.8.attn.resid_dropout Dropout(p=0.1, inplace=False)\n",
      "h.8.ln_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.8.mlp GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.8.mlp.c_fc Conv1D()\n",
      "h.8.mlp.c_proj Conv1D()\n",
      "h.8.mlp.act NewGELUActivation()\n",
      "h.8.mlp.dropout Dropout(p=0.1, inplace=False)\n",
      "h.9 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "h.9.ln_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.9.attn GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.9.attn.c_attn Conv1D()\n",
      "h.9.attn.c_proj Conv1D()\n",
      "h.9.attn.attn_dropout Dropout(p=0.1, inplace=False)\n",
      "h.9.attn.resid_dropout Dropout(p=0.1, inplace=False)\n",
      "h.9.ln_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.9.mlp GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.9.mlp.c_fc Conv1D()\n",
      "h.9.mlp.c_proj Conv1D()\n",
      "h.9.mlp.act NewGELUActivation()\n",
      "h.9.mlp.dropout Dropout(p=0.1, inplace=False)\n",
      "h.10 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "h.10.ln_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.10.attn GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.10.attn.c_attn Conv1D()\n",
      "h.10.attn.c_proj Conv1D()\n",
      "h.10.attn.attn_dropout Dropout(p=0.1, inplace=False)\n",
      "h.10.attn.resid_dropout Dropout(p=0.1, inplace=False)\n",
      "h.10.ln_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.10.mlp GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.10.mlp.c_fc Conv1D()\n",
      "h.10.mlp.c_proj Conv1D()\n",
      "h.10.mlp.act NewGELUActivation()\n",
      "h.10.mlp.dropout Dropout(p=0.1, inplace=False)\n",
      "h.11 GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "h.11.ln_1 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.11.attn GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.11.attn.c_attn Conv1D()\n",
      "h.11.attn.c_proj Conv1D()\n",
      "h.11.attn.attn_dropout Dropout(p=0.1, inplace=False)\n",
      "h.11.attn.resid_dropout Dropout(p=0.1, inplace=False)\n",
      "h.11.ln_2 LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "h.11.mlp GPT2MLP(\n",
      "  (c_fc): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (act): NewGELUActivation()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "h.11.mlp.c_fc Conv1D()\n",
      "h.11.mlp.c_proj Conv1D()\n",
      "h.11.mlp.act NewGELUActivation()\n",
      "h.11.mlp.dropout Dropout(p=0.1, inplace=False)\n",
      "ln_f LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer_gpt = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model_gpt = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "for name, module in model_gpt.named_modules():\n",
    "    print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grow_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'grow_depth' from '/Users/vibhamasti/Personal/CMU/S24/Capstone/code/LoRA-Instruction-Finetune/src/grow_depth.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(grow_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/Users/vibhamasti/Personal/CMU/S24/Capstone/code/LoRA-Instruction-Finetune/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finish the following sentence:\\nRaindrops on roses, and the flowers on the ground.\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_160m = GPTNeoXForCausalLM.from_pretrained(\n",
    "  \"EleutherAI/pythia-160m\",\n",
    "  cache_dir=\"../.cache/pythia-160m\",\n",
    ")\n",
    "\n",
    "tokenizer_160m = AutoTokenizer.from_pretrained(\n",
    "  \"EleutherAI/pythia-160m\",\n",
    "  cache_dir=\"../.cache/pythia-160m\",\n",
    ")\n",
    "\n",
    "inputs = tokenizer_160m(\"Finish the following sentence:\\nRaindrops on roses\", return_tensors=\"pt\")\n",
    "tokens = model_160m.generate(**inputs)\n",
    "tokenizer_160m.decode(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finish the following sentence:\\nRaindrops on roses, and the flowers on the ground.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer_70m(\"Finish the following sentence:\\nRaindrops on roses\", return_tensors=\"pt\")\n",
    "tokens = model_160m.generate(**inputs)\n",
    "tokenizer_70m.decode(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50304, 512)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_70m.gpt_neox.embed_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50304, 512])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_70m.gpt_neox.embed_in.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50304, 512])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_70m.embed_out.weight.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=50304, bias=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_70m.embed_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 512)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      ")\n",
      "gpt_neox GPTNeoXModel(\n",
      "  (embed_in): Embedding(50304, 512)\n",
      "  (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-5): 6 x GPTNeoXLayer(\n",
      "      (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (attention): GPTNeoXAttention(\n",
      "        (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "        (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (mlp): GPTNeoXMLP(\n",
      "        (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (act): GELUActivation()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "gpt_neox.embed_in Embedding(50304, 512)\n",
      "gpt_neox.emb_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers ModuleList(\n",
      "  (0-5): 6 x GPTNeoXLayer(\n",
      "    (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (attention): GPTNeoXAttention(\n",
      "      (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "      (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (mlp): GPTNeoXMLP(\n",
      "      (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (act): GELUActivation()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.0 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.0.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.0.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.0.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.0.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.0.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.0.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.0.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "gpt_neox.layers.0.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "gpt_neox.layers.0.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.0.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.0.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "gpt_neox.layers.0.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "gpt_neox.layers.0.mlp.act GELUActivation()\n",
      "gpt_neox.layers.1 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.1.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.1.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.1.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.1.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.1.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.1.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.1.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "gpt_neox.layers.1.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "gpt_neox.layers.1.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.1.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.1.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "gpt_neox.layers.1.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "gpt_neox.layers.1.mlp.act GELUActivation()\n",
      "gpt_neox.layers.2 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.2.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.2.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.2.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.2.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.2.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.2.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.2.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "gpt_neox.layers.2.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "gpt_neox.layers.2.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.2.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.2.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "gpt_neox.layers.2.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "gpt_neox.layers.2.mlp.act GELUActivation()\n",
      "gpt_neox.layers.3 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.3.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.3.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.3.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.3.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.3.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.3.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.3.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "gpt_neox.layers.3.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "gpt_neox.layers.3.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.3.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.3.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "gpt_neox.layers.3.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "gpt_neox.layers.3.mlp.act GELUActivation()\n",
      "gpt_neox.layers.4 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.4.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.4.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.4.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.4.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.4.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.4.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.4.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "gpt_neox.layers.4.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "gpt_neox.layers.4.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.4.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.4.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "gpt_neox.layers.4.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "gpt_neox.layers.4.mlp.act GELUActivation()\n",
      "gpt_neox.layers.5 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.5.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.5.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.5.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.5.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.5.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.5.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.5.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "gpt_neox.layers.5.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "gpt_neox.layers.5.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.5.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.5.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "gpt_neox.layers.5.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "gpt_neox.layers.5.mlp.act GELUActivation()\n",
      "gpt_neox.final_layer_norm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "embed_out Linear(in_features=512, out_features=50304, bias=False)\n"
     ]
    }
   ],
   "source": [
    "for name, module in model_70m.named_modules():\n",
    "    print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 768)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=768, out_features=50304, bias=False)\n",
      ")\n",
      "gpt_neox GPTNeoXModel(\n",
      "  (embed_in): Embedding(50304, 768)\n",
      "  (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-11): 12 x GPTNeoXLayer(\n",
      "      (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (attention): GPTNeoXAttention(\n",
      "        (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "        (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (mlp): GPTNeoXMLP(\n",
      "        (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (act): GELUActivation()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "gpt_neox.embed_in Embedding(50304, 768)\n",
      "gpt_neox.emb_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers ModuleList(\n",
      "  (0-11): 12 x GPTNeoXLayer(\n",
      "    (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (attention): GPTNeoXAttention(\n",
      "      (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "      (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (mlp): GPTNeoXMLP(\n",
      "      (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (act): GELUActivation()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.0 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.0.input_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.0.post_attention_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.0.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.0.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.0.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.0.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.0.attention.query_key_value Linear(in_features=768, out_features=2304, bias=True)\n",
      "gpt_neox.layers.0.attention.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "gpt_neox.layers.0.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.0.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.0.mlp.dense_h_to_4h Linear(in_features=768, out_features=3072, bias=True)\n",
      "gpt_neox.layers.0.mlp.dense_4h_to_h Linear(in_features=3072, out_features=768, bias=True)\n",
      "gpt_neox.layers.0.mlp.act GELUActivation()\n",
      "gpt_neox.layers.1 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.1.input_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.1.post_attention_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.1.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.1.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.1.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.1.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.1.attention.query_key_value Linear(in_features=768, out_features=2304, bias=True)\n",
      "gpt_neox.layers.1.attention.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "gpt_neox.layers.1.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.1.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.1.mlp.dense_h_to_4h Linear(in_features=768, out_features=3072, bias=True)\n",
      "gpt_neox.layers.1.mlp.dense_4h_to_h Linear(in_features=3072, out_features=768, bias=True)\n",
      "gpt_neox.layers.1.mlp.act GELUActivation()\n",
      "gpt_neox.layers.2 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.2.input_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.2.post_attention_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.2.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.2.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.2.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.2.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.2.attention.query_key_value Linear(in_features=768, out_features=2304, bias=True)\n",
      "gpt_neox.layers.2.attention.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "gpt_neox.layers.2.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.2.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.2.mlp.dense_h_to_4h Linear(in_features=768, out_features=3072, bias=True)\n",
      "gpt_neox.layers.2.mlp.dense_4h_to_h Linear(in_features=3072, out_features=768, bias=True)\n",
      "gpt_neox.layers.2.mlp.act GELUActivation()\n",
      "gpt_neox.layers.3 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.3.input_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.3.post_attention_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.3.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.3.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.3.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.3.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.3.attention.query_key_value Linear(in_features=768, out_features=2304, bias=True)\n",
      "gpt_neox.layers.3.attention.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "gpt_neox.layers.3.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.3.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.3.mlp.dense_h_to_4h Linear(in_features=768, out_features=3072, bias=True)\n",
      "gpt_neox.layers.3.mlp.dense_4h_to_h Linear(in_features=3072, out_features=768, bias=True)\n",
      "gpt_neox.layers.3.mlp.act GELUActivation()\n",
      "gpt_neox.layers.4 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.4.input_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.4.post_attention_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.4.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.4.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.4.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.4.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.4.attention.query_key_value Linear(in_features=768, out_features=2304, bias=True)\n",
      "gpt_neox.layers.4.attention.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "gpt_neox.layers.4.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.4.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.4.mlp.dense_h_to_4h Linear(in_features=768, out_features=3072, bias=True)\n",
      "gpt_neox.layers.4.mlp.dense_4h_to_h Linear(in_features=3072, out_features=768, bias=True)\n",
      "gpt_neox.layers.4.mlp.act GELUActivation()\n",
      "gpt_neox.layers.5 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.5.input_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.5.post_attention_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.5.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.5.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.5.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.5.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.5.attention.query_key_value Linear(in_features=768, out_features=2304, bias=True)\n",
      "gpt_neox.layers.5.attention.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "gpt_neox.layers.5.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.5.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.5.mlp.dense_h_to_4h Linear(in_features=768, out_features=3072, bias=True)\n",
      "gpt_neox.layers.5.mlp.dense_4h_to_h Linear(in_features=3072, out_features=768, bias=True)\n",
      "gpt_neox.layers.5.mlp.act GELUActivation()\n",
      "gpt_neox.layers.6 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.6.input_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.6.post_attention_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.6.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.6.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.6.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.6.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.6.attention.query_key_value Linear(in_features=768, out_features=2304, bias=True)\n",
      "gpt_neox.layers.6.attention.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "gpt_neox.layers.6.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.6.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.6.mlp.dense_h_to_4h Linear(in_features=768, out_features=3072, bias=True)\n",
      "gpt_neox.layers.6.mlp.dense_4h_to_h Linear(in_features=3072, out_features=768, bias=True)\n",
      "gpt_neox.layers.6.mlp.act GELUActivation()\n",
      "gpt_neox.layers.7 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.7.input_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.7.post_attention_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.7.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.7.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.7.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.7.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.7.attention.query_key_value Linear(in_features=768, out_features=2304, bias=True)\n",
      "gpt_neox.layers.7.attention.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "gpt_neox.layers.7.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.7.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.7.mlp.dense_h_to_4h Linear(in_features=768, out_features=3072, bias=True)\n",
      "gpt_neox.layers.7.mlp.dense_4h_to_h Linear(in_features=3072, out_features=768, bias=True)\n",
      "gpt_neox.layers.7.mlp.act GELUActivation()\n",
      "gpt_neox.layers.8 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.8.input_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.8.post_attention_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.8.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.8.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.8.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.8.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.8.attention.query_key_value Linear(in_features=768, out_features=2304, bias=True)\n",
      "gpt_neox.layers.8.attention.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "gpt_neox.layers.8.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.8.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.8.mlp.dense_h_to_4h Linear(in_features=768, out_features=3072, bias=True)\n",
      "gpt_neox.layers.8.mlp.dense_4h_to_h Linear(in_features=3072, out_features=768, bias=True)\n",
      "gpt_neox.layers.8.mlp.act GELUActivation()\n",
      "gpt_neox.layers.9 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.9.input_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.9.post_attention_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.9.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.9.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.9.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.9.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.9.attention.query_key_value Linear(in_features=768, out_features=2304, bias=True)\n",
      "gpt_neox.layers.9.attention.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "gpt_neox.layers.9.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.9.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.9.mlp.dense_h_to_4h Linear(in_features=768, out_features=3072, bias=True)\n",
      "gpt_neox.layers.9.mlp.dense_4h_to_h Linear(in_features=3072, out_features=768, bias=True)\n",
      "gpt_neox.layers.9.mlp.act GELUActivation()\n",
      "gpt_neox.layers.10 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.10.input_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.10.post_attention_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.10.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.10.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.10.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.10.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.10.attention.query_key_value Linear(in_features=768, out_features=2304, bias=True)\n",
      "gpt_neox.layers.10.attention.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "gpt_neox.layers.10.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.10.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.10.mlp.dense_h_to_4h Linear(in_features=768, out_features=3072, bias=True)\n",
      "gpt_neox.layers.10.mlp.dense_4h_to_h Linear(in_features=3072, out_features=768, bias=True)\n",
      "gpt_neox.layers.10.mlp.act GELUActivation()\n",
      "gpt_neox.layers.11 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.11.input_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.11.post_attention_layernorm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.11.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.11.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.11.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.11.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.11.attention.query_key_value Linear(in_features=768, out_features=2304, bias=True)\n",
      "gpt_neox.layers.11.attention.dense Linear(in_features=768, out_features=768, bias=True)\n",
      "gpt_neox.layers.11.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.11.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.11.mlp.dense_h_to_4h Linear(in_features=768, out_features=3072, bias=True)\n",
      "gpt_neox.layers.11.mlp.dense_4h_to_h Linear(in_features=3072, out_features=768, bias=True)\n",
      "gpt_neox.layers.11.mlp.act GELUActivation()\n",
      "gpt_neox.final_layer_norm LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "embed_out Linear(in_features=768, out_features=50304, bias=False)\n"
     ]
    }
   ],
   "source": [
    "for name, module in model_160m.named_modules():\n",
    "    print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_70m.gpt_neox.layers[0].input_layernorm.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 512)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      ")\n",
      "gpt_neox GPTNeoXModel(\n",
      "  (embed_in): Embedding(50304, 512)\n",
      "  (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-5): 6 x GPTNeoXLayer(\n",
      "      (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (attention): GPTNeoXAttention(\n",
      "        (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "        (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (mlp): GPTNeoXMLP(\n",
      "        (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (act): GELUActivation()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "gpt_neox.embed_in Embedding(50304, 512)\n",
      "gpt_neox.emb_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers ModuleList(\n",
      "  (0-5): 6 x GPTNeoXLayer(\n",
      "    (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (attention): GPTNeoXAttention(\n",
      "      (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "      (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (mlp): GPTNeoXMLP(\n",
      "      (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (act): GELUActivation()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.0 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.0.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.0.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.0.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.0.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.0.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.0.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.0.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "gpt_neox.layers.0.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "gpt_neox.layers.0.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.0.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.0.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "gpt_neox.layers.0.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "gpt_neox.layers.0.mlp.act GELUActivation()\n",
      "gpt_neox.layers.1 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.1.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.1.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.1.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.1.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.1.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.1.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.1.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "gpt_neox.layers.1.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "gpt_neox.layers.1.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.1.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.1.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "gpt_neox.layers.1.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "gpt_neox.layers.1.mlp.act GELUActivation()\n",
      "gpt_neox.layers.2 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.2.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.2.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.2.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.2.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.2.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.2.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.2.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "gpt_neox.layers.2.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "gpt_neox.layers.2.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.2.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.2.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "gpt_neox.layers.2.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "gpt_neox.layers.2.mlp.act GELUActivation()\n",
      "gpt_neox.layers.3 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.3.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.3.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.3.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.3.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.3.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.3.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.3.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "gpt_neox.layers.3.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "gpt_neox.layers.3.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.3.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.3.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "gpt_neox.layers.3.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "gpt_neox.layers.3.mlp.act GELUActivation()\n",
      "gpt_neox.layers.4 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.4.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.4.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.4.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.4.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.4.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.4.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.4.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "gpt_neox.layers.4.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "gpt_neox.layers.4.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.4.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.4.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "gpt_neox.layers.4.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "gpt_neox.layers.4.mlp.act GELUActivation()\n",
      "gpt_neox.layers.5 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "gpt_neox.layers.5.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.5.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "gpt_neox.layers.5.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.5.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.5.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "gpt_neox.layers.5.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "gpt_neox.layers.5.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "gpt_neox.layers.5.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "gpt_neox.layers.5.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "gpt_neox.layers.5.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "gpt_neox.layers.5.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "gpt_neox.layers.5.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "gpt_neox.layers.5.mlp.act GELUActivation()\n",
      "gpt_neox.final_layer_norm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "embed_out Linear(in_features=512, out_features=50304, bias=False)\n"
     ]
    }
   ],
   "source": [
    "for name, module in model_70m.named_modules():\n",
    "    print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0-11): 12 x GPTNeoXLayer(\n",
      "    (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (attention): GPTNeoXAttention(\n",
      "      (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "      (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (mlp): GPTNeoXMLP(\n",
      "      (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (act): GELUActivation()\n",
      "    )\n",
      "  )\n",
      ")\n",
      " ModuleList(\n",
      "  (0-11): 12 x GPTNeoXLayer(\n",
      "    (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (attention): GPTNeoXAttention(\n",
      "      (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "      (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (mlp): GPTNeoXMLP(\n",
      "      (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (act): GELUActivation()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "0 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "0.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "0.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "0.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "0.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "0.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "0.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "0.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "0.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "0.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "0.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "0.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "0.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "0.mlp.act GELUActivation()\n",
      "1 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "1.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "1.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "1.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "1.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "1.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "1.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "1.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "1.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "1.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "1.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "1.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "1.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "1.mlp.act GELUActivation()\n",
      "2 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "2.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "2.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "2.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "2.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "2.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "2.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "2.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "2.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "2.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "2.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "2.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "2.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "2.mlp.act GELUActivation()\n",
      "3 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "3.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "3.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "3.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "3.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "3.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "3.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "3.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "3.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "3.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "3.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "3.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "3.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "3.mlp.act GELUActivation()\n",
      "4 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "4.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "4.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "4.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "4.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "4.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "4.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "4.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "4.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "4.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "4.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "4.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "4.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "4.mlp.act GELUActivation()\n",
      "5 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "5.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "5.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "5.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "5.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "5.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "5.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "5.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "5.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "5.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "5.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "5.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "5.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "5.mlp.act GELUActivation()\n",
      "6 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "6.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "6.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "6.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "6.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "6.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "6.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "6.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "6.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "6.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "6.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "6.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "6.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "6.mlp.act GELUActivation()\n",
      "7 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "7.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "7.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "7.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "7.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "7.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "7.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "7.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "7.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "7.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "7.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "7.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "7.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "7.mlp.act GELUActivation()\n",
      "8 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "8.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "8.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "8.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "8.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "8.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "8.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "8.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "8.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "8.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "8.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "8.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "8.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "8.mlp.act GELUActivation()\n",
      "9 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "9.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "9.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "9.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "9.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "9.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "9.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "9.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "9.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "9.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "9.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "9.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "9.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "9.mlp.act GELUActivation()\n",
      "10 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "10.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "10.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "10.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "10.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "10.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "10.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "10.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "10.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "10.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "10.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "10.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "10.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "10.mlp.act GELUActivation()\n",
      "11 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ")\n",
      "11.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "11.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "11.post_attention_dropout Dropout(p=0.0, inplace=False)\n",
      "11.post_mlp_dropout Dropout(p=0.0, inplace=False)\n",
      "11.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "11.attention.rotary_emb GPTNeoXRotaryEmbedding()\n",
      "11.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True)\n",
      "11.attention.dense Linear(in_features=512, out_features=512, bias=True)\n",
      "11.attention.attention_dropout Dropout(p=0.0, inplace=False)\n",
      "11.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ")\n",
      "11.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True)\n",
      "11.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True)\n",
      "11.mlp.act GELUActivation()\n"
     ]
    }
   ],
   "source": [
    "model_70m_expanded = grow_depth.expand_layers(model_70m, 6, 12, expand_type='alternate')\n",
    "\n",
    "for name, module in model_70m_expanded.gpt_neox.layers.named_modules():\n",
    "    print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/Users/vibhamasti/Personal/CMU/S24/Capstone/code/LoRA-Instruction-Finetune/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finish the following sentence:\\nRaindrops on roses\\n\\nI have a question for you.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer_70m(\"Finish the following sentence:\\nRaindrops on roses\", return_tensors=\"pt\")\n",
    "tokens = model_70m_expanded.generate(**inputs)\n",
    "tokenizer_70m.decode(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 512)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      ") []\n",
      "gpt_neox GPTNeoXModel(\n",
      "  (embed_in): Embedding(50304, 512)\n",
      "  (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-5): 6 x GPTNeoXLayer(\n",
      "      (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (attention): GPTNeoXAttention(\n",
      "        (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "        (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (mlp): GPTNeoXMLP(\n",
      "        (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (act): GELUActivation()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ") []\n",
      "gpt_neox.embed_in Embedding(50304, 512) []\n",
      "gpt_neox.emb_dropout Dropout(p=0.0, inplace=False) []\n",
      "gpt_neox.layers ModuleList(\n",
      "  (0-5): 6 x GPTNeoXLayer(\n",
      "    (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (attention): GPTNeoXAttention(\n",
      "      (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "      (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (mlp): GPTNeoXMLP(\n",
      "      (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (act): GELUActivation()\n",
      "    )\n",
      "  )\n",
      ") []\n",
      "gpt_neox.layers.0 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ") ['0']\n",
      "gpt_neox.layers.0.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True) ['0']\n",
      "gpt_neox.layers.0.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True) ['0']\n",
      "gpt_neox.layers.0.post_attention_dropout Dropout(p=0.0, inplace=False) ['0']\n",
      "gpt_neox.layers.0.post_mlp_dropout Dropout(p=0.0, inplace=False) ['0']\n",
      "gpt_neox.layers.0.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ") ['0']\n",
      "gpt_neox.layers.0.attention.rotary_emb GPTNeoXRotaryEmbedding() ['0']\n",
      "gpt_neox.layers.0.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True) ['0']\n",
      "gpt_neox.layers.0.attention.dense Linear(in_features=512, out_features=512, bias=True) ['0']\n",
      "gpt_neox.layers.0.attention.attention_dropout Dropout(p=0.0, inplace=False) ['0']\n",
      "gpt_neox.layers.0.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ") ['0']\n",
      "gpt_neox.layers.0.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True) ['0', '4']\n",
      "gpt_neox.layers.0.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True) ['0', '4']\n",
      "gpt_neox.layers.0.mlp.act GELUActivation() ['0']\n",
      "gpt_neox.layers.1 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ") ['1']\n",
      "gpt_neox.layers.1.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True) ['1']\n",
      "gpt_neox.layers.1.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True) ['1']\n",
      "gpt_neox.layers.1.post_attention_dropout Dropout(p=0.0, inplace=False) ['1']\n",
      "gpt_neox.layers.1.post_mlp_dropout Dropout(p=0.0, inplace=False) ['1']\n",
      "gpt_neox.layers.1.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ") ['1']\n",
      "gpt_neox.layers.1.attention.rotary_emb GPTNeoXRotaryEmbedding() ['1']\n",
      "gpt_neox.layers.1.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True) ['1']\n",
      "gpt_neox.layers.1.attention.dense Linear(in_features=512, out_features=512, bias=True) ['1']\n",
      "gpt_neox.layers.1.attention.attention_dropout Dropout(p=0.0, inplace=False) ['1']\n",
      "gpt_neox.layers.1.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ") ['1']\n",
      "gpt_neox.layers.1.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True) ['1', '4']\n",
      "gpt_neox.layers.1.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True) ['1', '4']\n",
      "gpt_neox.layers.1.mlp.act GELUActivation() ['1']\n",
      "gpt_neox.layers.2 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ") ['2']\n",
      "gpt_neox.layers.2.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True) ['2']\n",
      "gpt_neox.layers.2.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True) ['2']\n",
      "gpt_neox.layers.2.post_attention_dropout Dropout(p=0.0, inplace=False) ['2']\n",
      "gpt_neox.layers.2.post_mlp_dropout Dropout(p=0.0, inplace=False) ['2']\n",
      "gpt_neox.layers.2.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ") ['2']\n",
      "gpt_neox.layers.2.attention.rotary_emb GPTNeoXRotaryEmbedding() ['2']\n",
      "gpt_neox.layers.2.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True) ['2']\n",
      "gpt_neox.layers.2.attention.dense Linear(in_features=512, out_features=512, bias=True) ['2']\n",
      "gpt_neox.layers.2.attention.attention_dropout Dropout(p=0.0, inplace=False) ['2']\n",
      "gpt_neox.layers.2.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ") ['2']\n",
      "gpt_neox.layers.2.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True) ['2', '4']\n",
      "gpt_neox.layers.2.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True) ['2', '4']\n",
      "gpt_neox.layers.2.mlp.act GELUActivation() ['2']\n",
      "gpt_neox.layers.3 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ") ['3']\n",
      "gpt_neox.layers.3.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True) ['3']\n",
      "gpt_neox.layers.3.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True) ['3']\n",
      "gpt_neox.layers.3.post_attention_dropout Dropout(p=0.0, inplace=False) ['3']\n",
      "gpt_neox.layers.3.post_mlp_dropout Dropout(p=0.0, inplace=False) ['3']\n",
      "gpt_neox.layers.3.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ") ['3']\n",
      "gpt_neox.layers.3.attention.rotary_emb GPTNeoXRotaryEmbedding() ['3']\n",
      "gpt_neox.layers.3.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True) ['3']\n",
      "gpt_neox.layers.3.attention.dense Linear(in_features=512, out_features=512, bias=True) ['3']\n",
      "gpt_neox.layers.3.attention.attention_dropout Dropout(p=0.0, inplace=False) ['3']\n",
      "gpt_neox.layers.3.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ") ['3']\n",
      "gpt_neox.layers.3.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True) ['3', '4']\n",
      "gpt_neox.layers.3.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True) ['3', '4']\n",
      "gpt_neox.layers.3.mlp.act GELUActivation() ['3']\n",
      "gpt_neox.layers.4 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ") ['4']\n",
      "gpt_neox.layers.4.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True) ['4']\n",
      "gpt_neox.layers.4.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True) ['4']\n",
      "gpt_neox.layers.4.post_attention_dropout Dropout(p=0.0, inplace=False) ['4']\n",
      "gpt_neox.layers.4.post_mlp_dropout Dropout(p=0.0, inplace=False) ['4']\n",
      "gpt_neox.layers.4.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ") ['4']\n",
      "gpt_neox.layers.4.attention.rotary_emb GPTNeoXRotaryEmbedding() ['4']\n",
      "gpt_neox.layers.4.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True) ['4']\n",
      "gpt_neox.layers.4.attention.dense Linear(in_features=512, out_features=512, bias=True) ['4']\n",
      "gpt_neox.layers.4.attention.attention_dropout Dropout(p=0.0, inplace=False) ['4']\n",
      "gpt_neox.layers.4.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ") ['4']\n",
      "gpt_neox.layers.4.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True) ['4', '4']\n",
      "gpt_neox.layers.4.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True) ['4', '4']\n",
      "gpt_neox.layers.4.mlp.act GELUActivation() ['4']\n",
      "gpt_neox.layers.5 GPTNeoXLayer(\n",
      "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (attention): GPTNeoXAttention(\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (mlp): GPTNeoXMLP(\n",
      "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (act): GELUActivation()\n",
      "  )\n",
      ") ['5']\n",
      "gpt_neox.layers.5.input_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True) ['5']\n",
      "gpt_neox.layers.5.post_attention_layernorm LayerNorm((512,), eps=1e-05, elementwise_affine=True) ['5']\n",
      "gpt_neox.layers.5.post_attention_dropout Dropout(p=0.0, inplace=False) ['5']\n",
      "gpt_neox.layers.5.post_mlp_dropout Dropout(p=0.0, inplace=False) ['5']\n",
      "gpt_neox.layers.5.attention GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ") ['5']\n",
      "gpt_neox.layers.5.attention.rotary_emb GPTNeoXRotaryEmbedding() ['5']\n",
      "gpt_neox.layers.5.attention.query_key_value Linear(in_features=512, out_features=1536, bias=True) ['5']\n",
      "gpt_neox.layers.5.attention.dense Linear(in_features=512, out_features=512, bias=True) ['5']\n",
      "gpt_neox.layers.5.attention.attention_dropout Dropout(p=0.0, inplace=False) ['5']\n",
      "gpt_neox.layers.5.mlp GPTNeoXMLP(\n",
      "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (act): GELUActivation()\n",
      ") ['5']\n",
      "gpt_neox.layers.5.mlp.dense_h_to_4h Linear(in_features=512, out_features=2048, bias=True) ['5', '4']\n",
      "gpt_neox.layers.5.mlp.dense_4h_to_h Linear(in_features=2048, out_features=512, bias=True) ['5', '4']\n",
      "gpt_neox.layers.5.mlp.act GELUActivation() ['5']\n",
      "gpt_neox.final_layer_norm LayerNorm((512,), eps=1e-05, elementwise_affine=True) []\n",
      "embed_out Linear(in_features=512, out_features=50304, bias=False) []\n"
     ]
    }
   ],
   "source": [
    "for name, module in model_70m.named_modules():\n",
    "\n",
    "  layer_idx = re.findall(\"[-\\d]+\", name)\n",
    "  print(name, module, layer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_depth(model):\n",
    "    print(model)\n",
    "    config = model.config\n",
    "\n",
    "    # create an instance of the model twice the size\n",
    "    new_config_dict = config.to_dict()\n",
    "    print(new_config_dict)\n",
    "    new_config_dict['num_hidden_layers'] *= 2\n",
    "\n",
    "    # new_config = type(config).from_dict(new_config_dict)\n",
    "    # new_model = type(model)(new_config)\n",
    "\n",
    "    # # load the weights from the old model into new model after duplicating them\n",
    "    # # model.tie_weights()\n",
    "    # # new_model.tie_weights()\n",
    "\n",
    "    # new_state_dict = deep_state_dict(model.state_dict())\n",
    "    # new_model.load_state_dict(new_state_dict)\n",
    "    # # new_model.tie_weights()\n",
    "\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finish the following sentence:\\nRaindrops on roses, and the flowers on the ground.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_160m = GPTNeoXForCausalLM.from_pretrained(\n",
    "  \"EleutherAI/pythia-160m\",\n",
    "  cache_dir=\"../.cache/pythia-160m\",\n",
    ")\n",
    "\n",
    "tokenizer_160m = AutoTokenizer.from_pretrained(\n",
    "  \"EleutherAI/pythia-160m\",\n",
    "  cache_dir=\"../.cache/pythia-160m\",\n",
    ")\n",
    "\n",
    "inputs = tokenizer_160m(\"Finish the following sentence:\\nRaindrops on roses\", return_tensors=\"pt\")\n",
    "tokens = model_160m.generate(**inputs)\n",
    "tokenizer_160m.decode(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
